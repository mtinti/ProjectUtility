[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Utility",
    "section": "",
    "text": "This project is a collection of essential functions and utilities that I frequently use in my data analysis workflows.",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#why-this-project-exists",
    "href": "index.html#why-this-project-exists",
    "title": "Project Utility",
    "section": "Why This Project Exists",
    "text": "Why This Project Exists\nAs a data analyst, I found myself repeatedly writing the same helper functions across different projects. ProjectUtility centralizes these common utilities into a single, well-documented package that can be easily imported into any analysis.\n\nDocumentation\n\n\n\nInstall ProjectUtility in Development mode\nIf you are new to using nbdev here are some useful pointers to get you started.\n# make sure ProjectUtility package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to ProjectUtility\n$ nbdev_prepare",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Project Utility",
    "section": "Usage",
    "text": "Usage\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/mtinti/ProjectUtility.git\nor from conda\n$ conda install -c mtinti ProjectUtility\nor from pypi\n$ pip install ProjectUtility",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#question",
    "href": "index.html#question",
    "title": "Project Utility",
    "section": "üîç Question",
    "text": "üîç Question\n\nCan you visualize the pattern of missing values in my dataset?\n\n\nfrom ProjectUtility import mis_val_utility\nimport pandas as pd\ndf = pd.read_csv('../tests/missing_values/test_df.csv.gz',index_col=[0])\nmv_analyzer = mis_val_utility.MissingValuesAnalyzer(df)\nfig, axes, summary = mv_analyzer.plot_missing_dashboard(figsize=(10, 12))\n\n/Users/MTinti/miniconda3/envs/work3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n  from pandas.core import (",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#question-1",
    "href": "index.html#question-1",
    "title": "Project Utility",
    "section": "üîç Question",
    "text": "üîç Question\n\nCan you reduce the dimensionality of my dataset to visualize patterns and relationships?\n\n\nfrom ProjectUtility import dim_red_utility\nimport matplotlib.pyplot as plt\nfrom ProjectUtility.core import convert_palette_to_hex, create_group_color_mapping\n\n\nsample_groups, color_dictionary = create_group_color_mapping(df.columns, \n                                                             group_size=3, \n                                                             return_color_to_group=True)\n\ncolor_dictionary = {'#FF5733': 'B1', '#33FF57': 'C3', '#3357FF': 'WT'}\n\nfig, axes, results_dict = dim_red_utility.create_dim_reduction_dashboard(\n    in_df=df.dropna(),\n    sample_palette=sample_groups,\n    feature_palette={},\n    top=500,\n    color_dictionary=color_dictionary,\n    title=\"Dim Red Analysis\",\n    figsize=(12, 10)  \n)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nExplained variance ratio: [0.81546122 0.12599201 0.02508285 0.01313488 0.00740248]",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#question-2",
    "href": "index.html#question-2",
    "title": "Project Utility",
    "section": "üîç Question",
    "text": "üîç Question\n\nCan you assess the reproducibility of my dataset?\n\n\nfrom ProjectUtility import correlation_utilities\nimport numpy as np\ncv_sample_groups = [\n    'B1', 'B1', 'B1', \n    'C3', 'C3', 'C3',\n    'WT', 'WT', 'WT']\nanalyzer = correlation_utilities.ReplicateAnalyzer(\n    np.log10(df.dropna()), cv_sample_groups)\n# Calculate coefficient of variation\ncv_results = analyzer.calculate_coefficient_of_variation()\nprint(\"Coefficient of Variation Results:\")\nprint(cv_results)\n\n# Visualize the results\nfig = analyzer.plot_coefficient_of_variation(\n    title=\"CV Comparison Between Sample Groups\",\n    figsize=(8, 5)\n)\n\nCoefficient of Variation Results:\n{'WT': 1.5810533161044804, 'B1': 1.17165193540931, 'C3': 1.191373053916986}\n\n\n\n\n\n\n\n\n\n\nCan you do the same row wise?\n\n\ncv_distribution = analyzer.calculate_cv_distribution(exclude_zeros=True)\n# Create the CV boxplot\nprint(\"\\nGenerating CV distribution boxplot...\")\nfig2 = analyzer.plot_cv_boxplot(\n    min_y=0,  # Minimum y-axis value\n    max_y=5,  # Maximum y-axis value\n    figsize=(6, 6),\n    color_palette=\"viridis\",\n    display_median=True,\n    reference_line=3,  \n    title=\"Distribution of Coefficient of Variation Across Sample Groups\"\n)\n\n# Show plots\nplt.tight_layout()\nplt.show()\n\n\nGenerating CV distribution boxplot...",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#question-3",
    "href": "index.html#question-3",
    "title": "Project Utility",
    "section": "üîç Question",
    "text": "üîç Question\n\nCan you show me an interactive volcan plot?\n\n\nfrom ProjectUtility import diff_expr\ncolumn_mapping = {\n    'log2fc': 'logFC',            # logFC column from your data\n    'fdr': 'FDR',                # FDR column from your data\n    'avg_intensity': 'log_AveExpr', # log_AveExpr column from your data\n    'id': 'Gene_id',            # Gene_acc column from your data\n    'description': 'Desc'         # Desc column from your data\n}\n\nfile_path = '../tests/volcano_plots/for_web_limma_WT-C3.csv.zip'\n# Create the PlotData instance\nplot_data = diff_expr.PlotData(file_path, column_mapping,\n                               highlight_ids=['Blasticidin','Puromycin'])\n# Quick access to all plotting data\n_ = plot_data.get_data_for_plotting()\n\n\nfig = diff_expr.create_volcano_ma_plots(\n    plot_data,\n    plot_title=\"Differential Expression Analysis: Sample vs Control\"\n)\n\n\nfig.show('iframe')\n\n\n\n\n\n\nyou can play with the interactive output here: https://mtinti.github.io/ProjectUtility/",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#question-4",
    "href": "index.html#question-4",
    "title": "Project Utility",
    "section": "üîç Question",
    "text": "üîç Question\n\nCan you analyse the identification rate of my proteomics sample?\n\n\nimport polars as pl\nfrom ProjectUtility import proteomics \nfname = '../tests/proteomics/dia_nn_report.tsv.gz'\ntmp = pl.read_csv(fname,separator='\\t')\ncount_data= proteomics.prepare_data_DiaNN(tmp)\ncount_data=count_data.to_pandas()\n_ = proteomics.plot_proteomics_run(count_data,\n                               figsize=(8,4))",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "index.html#documentation-1",
    "href": "index.html#documentation-1",
    "title": "Project Utility",
    "section": "Documentation",
    "text": "Documentation\nDocumentation can be found hosted on this GitHub repository‚Äôs pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "Project Utility"
    ]
  },
  {
    "objectID": "dim_red_utility.html",
    "href": "dim_red_utility.html",
    "title": "Dimensionality Reduction Utilities",
    "section": "",
    "text": "Flexible utilities for PCA, MDS, and biplots visualizations.",
    "crumbs": [
      "Dimensionality Reduction Utilities"
    ]
  },
  {
    "objectID": "dim_red_utility.html#the-plotter-class",
    "href": "dim_red_utility.html#the-plotter-class",
    "title": "Dimensionality Reduction Utilities",
    "section": "The Plotter Class",
    "text": "The Plotter Class\nThis class provides a unified framework for dimensionality reduction techniques, with a focus on visualization\n\nsource\n\nDimensionalityReductionPlotter\n\n DimensionalityReductionPlotter (in_df:pandas.core.frame.DataFrame,\n                                 top:int=500, color_dictionary:Optional[Di\n                                 ct[str,str]]=None)\n\nA class for creating dimensionality reduction plots (PCA, MDS) from pandas DataFrames.\n\n# Create synthetic data\nnp.random.seed(42)\n\n# Number of samples and features\nn_samples = 50\nn_features = 100\n\n# Generate random data with two distinct groups and some structure\ndata = np.random.randn(n_features, n_samples)\n\n# Add some structure - make the first 30 features higher in first 25 samples\ndata[:30, :25] += 2\n\n# Create a pandas DataFrame\nfeature_names = [f'feature_{i}' for i in range(n_features)]\nsample_names = [f'sample_{i}' for i in range(n_samples)]\ndf = pd.DataFrame(data, index=feature_names, columns=sample_names)\n\n# Create sample groups - mapping each sample to its color\nsample_groups = {}\nfor i in range(n_samples):\n    if i &lt; 25:\n        sample_groups[f'sample_{i}'] = 'red'\n    else:\n        sample_groups[f'sample_{i}'] = 'blue'\n\n# Create feature groups - mapping each feature to its color\nfeature_groups = {}\nfor i in range(n_features):\n    if i &lt; 30:\n        feature_groups[f'feature_{i}'] = 'green'\n    elif i &lt; 60:\n        feature_groups[f'feature_{i}'] = 'purple'\n    else:\n        feature_groups[f'feature_{i}'] = 'orange'\n\n# Create a color dictionary for nice labels\ncolor_dict = {\n    'red': 'Group A',\n    'blue': 'Group B',\n    'green': 'Gene Set 1',\n    'purple': 'Gene Set 2',\n    'orange': 'Gene Set 3'\n}\n\n# Preview the data\ndf.iloc[:5, :5]\n\n\n\n\n\n\n\n\nsample_0\nsample_1\nsample_2\nsample_3\nsample_4\n\n\n\n\nfeature_0\n2.496714\n1.861736\n2.647689\n3.523030\n1.765847\n\n\nfeature_1\n2.324084\n1.614918\n1.323078\n2.611676\n3.031000\n\n\nfeature_2\n0.584629\n1.579355\n1.657285\n1.197723\n1.838714\n\n\nfeature_3\n2.250493\n2.346448\n1.319975\n2.232254\n2.293072\n\n\nfeature_4\n2.357787\n2.560785\n3.083051\n3.053802\n0.622331\n\n\n\n\n\n\n\n\n# Create a plotter instance\nplotter = DimensionalityReductionPlotter(\n    in_df=df,\n    top=50,  # Use top 50 features\n    color_dictionary=color_dict\n)\n\n# Fit PCA and plot samples\nplotter.fit(method='pca', n_components=5)\nfig, ax, tmp_df = plotter.plot_samples(\n    palette=sample_groups,\n    point_size=80,\n    do_adjust_text=False,\n    title=\"PCA of Synthetic Data\"\n)\ntmp_df.iloc[:5, :5]\n\nExplained variance ratio: [0.42735199 0.04923835 0.04162556 0.0367032  0.0323377 ]\n\n\n\n\n\n\n\n\n\npc_1\npc_2\npc_3\npc_4\npc_5\n\n\n\n\nsample_0\n4.833824\n-0.524305\n-1.099760\n-1.171201\n0.348957\n\n\nsample_1\n4.922518\n-2.195776\n-3.805798\n-2.724017\n-2.896950\n\n\nsample_2\n5.531237\n2.777291\n-0.293219\n-2.368883\n-0.057843\n\n\nsample_3\n7.213643\n1.604138\n-2.255550\n-1.751800\n2.053669\n\n\nsample_4\n5.982604\n-0.348769\n-0.019634\n1.754475\n0.805486\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the feature loadings as arrows\nfig, ax, tmp_df  = plotter.plot_loadings(\n    palette=feature_groups,\n    arrow=True,\n    arrow_scale=3,\n    title=\"PCA Feature Loadings\",\n    biggest=2\n)\ntmp_df.iloc[:5, :5]\n\n\n\n\n\n\n\n\npc_1\npc_2\npc_3\npc_4\npc_5\n\n\n\n\nfeature_56\n-0.009073\n-0.075461\n0.134354\n-0.212345\n-0.081578\n\n\nfeature_70\n-0.037480\n0.082879\n-0.150355\n-0.106533\n0.243716\n\n\nfeature_99\n-0.016096\n0.101830\n-0.366043\n-0.102091\n-0.031489\n\n\nfeature_71\n-0.014748\n0.209104\n0.073929\n-0.087332\n0.088316\n\n\nfeature_94\n0.039709\n-0.044942\n0.003832\n-0.093297\n0.106068\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a biplot\nfig, ax, tmp_df_dict = plotter.plot_biplot(\n    feature_palette=feature_groups,\n    sample_palette=sample_groups,\n    arrow_scale=4,\n    sample_size=70,\n    title=\"PCA Biplot of Synthetic Data\",\n    #biggest=2\n)\n#tmp_df.iloc[:5, :5]\nprint(tmp_df_dict['samples'].iloc[:5, :5])\nprint(tmp_df_dict['features'].iloc[:5, :5])\n\n              pc_1      pc_2      pc_3      pc_4      pc_5\nsample_0  4.833824 -0.524305 -1.099760 -1.171201  0.348957\nsample_1  4.922518 -2.195776 -3.805798 -2.724017 -2.896950\nsample_2  5.531237  2.777291 -0.293219 -2.368883 -0.057843\nsample_3  7.213643  1.604138 -2.255550 -1.751800  2.053669\nsample_4  5.982604 -0.348769 -0.019634  1.754475  0.805486\n                pc_1      pc_2      pc_3      pc_4      pc_5\nfeature_56 -0.009073 -0.075461  0.134354 -0.212345 -0.081578\nfeature_70 -0.037480  0.082879 -0.150355 -0.106533  0.243716\nfeature_99 -0.016096  0.101830 -0.366043 -0.102091 -0.031489\nfeature_71 -0.014748  0.209104  0.073929 -0.087332  0.088316\nfeature_94  0.039709 -0.044942  0.003832 -0.093297  0.106068\n\n\n\n\n\n\n\n\n\n\n# Cumulative explained variance\nfig, ax, tmp_df = plotter.plot_explained_variance(\n    cumulative=True,\n    color=\"#9B1D20\"\n)\ntmp_df.head()\n\n\n\n\n\n\n\n\ncomponent\nexplained_variance\ncumulative_variance\n\n\n\n\n0\n1\n42.735199\n42.735199\n\n\n1\n2\n4.923835\n47.659034\n\n\n2\n3\n4.162556\n51.821590\n\n\n3\n4\n3.670320\n55.491911\n\n\n4\n5\n3.233770\n58.725681\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Individual explained variance\nfig, ax, tmp_df = plotter.plot_explained_variance(\n    cumulative=False,\n    color=\"#1E88E5\",\n    title=\"Individual Variance Explained per Principal Component\"\n)\ntmp_df.head()\n\n\n\n\n\n\n\n\ncomponent\nexplained_variance\ncumulative_variance\n\n\n\n\n0\n1\n42.735199\n42.735199\n\n\n1\n2\n4.923835\n47.659034\n\n\n2\n3\n4.162556\n51.821590\n\n\n3\n4\n3.670320\n55.491911\n\n\n4\n5\n3.233770\n58.725681\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Switch to MDS\nplotter.fit(method='mds', metric=True, random_state=42)\n\n# Plot MDS results\nfig, ax, tmp_df = plotter.plot_samples(\n    palette=sample_groups,\n    point_size=80,\n    title=\"MDS of Synthetic Data\"\n)\ntmp_df.head()\n\n/Users/MTinti/miniconda3/envs/work3/lib/python3.10/site-packages/sklearn/manifold/_mds.py:517: UserWarning: The MDS API has changed. ``fit`` now constructs an dissimilarity matrix from data. To use a custom dissimilarity matrix, set ``dissimilarity='precomputed'``.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\ndim_1\ndim_2\ncolor\n\n\n\n\nsample_0\n-2.463140\n4.037141\nred\n\n\nsample_1\n-8.969954\n3.636025\nred\n\n\nsample_2\n0.072362\n6.270072\nred\n\n\nsample_3\n-3.341935\n10.120119\nred\n\n\nsample_4\n-3.417970\n6.328873\nred\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ncreate_dim_reduction_dashboard\n\n create_dim_reduction_dashboard (in_df, sample_palette, feature_palette,\n                                 top=50, color_dictionary=None,\n                                 n_components=5, title='Dimensionality\n                                 Reduction Dashboard', figsize=(16, 12))\n\nCreate a comprehensive 2x2 dashboard of dimensionality reduction visualizations.\n\n# Create the dashboard\nfig, axes, results_dict = create_dim_reduction_dashboard(\n    in_df=df,\n    sample_palette=sample_groups,\n    feature_palette=feature_groups,\n    top=50,\n    color_dictionary=color_dict,\n    title=\"Synthetic Data Analysis Dashboard\"\n)\n\n# Fine tune the figure if needed\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to accommodate suptitle and caption\n\n# Now you have access to all the DataFrames for further analysis\nprint(\"Available DataFrames in results_dict:\")\nfor key in results_dict:\n    print(f\"- {key}: {results_dict[key].shape}\")\n    \n# Example of further analysis with the returned DataFrames\nprint(\"\\nExamined variance explained by first 3 components:\")\nprint(results_dict['explained_variance'].head(3))\n\nExplained variance ratio: [0.42735199 0.04923835 0.04162556 0.0367032  0.0323377 ]\n\n\n/Users/MTinti/miniconda3/envs/work3/lib/python3.10/site-packages/sklearn/manifold/_mds.py:517: UserWarning: The MDS API has changed. ``fit`` now constructs an dissimilarity matrix from data. To use a custom dissimilarity matrix, set ``dissimilarity='precomputed'``.\n  warnings.warn(\n\n\nAvailable DataFrames in results_dict:\n- pca_samples: (50, 6)\n- pca_loadings: (50, 5)\n- explained_variance: (5, 3)\n- mds_samples: (50, 3)\n\nExamined variance explained by first 3 components:\n   component  explained_variance  cumulative_variance\n0          1           42.735199            42.735199\n1          2            4.923835            47.659034\n2          3            4.162556            51.821590",
    "crumbs": [
      "Dimensionality Reduction Utilities"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Semi Random Collection of functions",
    "section": "",
    "text": "source\n\nconvert_palette_to_hex\n\n convert_palette_to_hex (palette_name, n_colors)\n\nConvert a named color palette to hex color codes.\n\nsource\n\n\ncreate_group_color_mapping\n\n create_group_color_mapping (items, group_size=3, palette=None,\n                             palette_name=None,\n                             return_color_to_group=False)\n\nCreate a color mapping dictionary that assigns the same color to items in groups.\n\n# Example usage with different palette options\ndef demonstrate_group_mapping_palettes():\n    # Create a list of items\n    items = [f'gene_{i}' for i in range(15)]\n    \n    # Create a figure with multiple palette examples\n    \n    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n    \n    # Example 1: Default palette\n    color_map1, group_map1 = create_group_color_mapping(\n        items, group_size=3, return_color_to_group=True\n    )\n    \n    # Example 2: tab10 palette\n    color_map2, group_map2 = create_group_color_mapping(\n        items, group_size=3, palette_name='tab10', return_color_to_group=True\n    )\n    \n    # Example 3: Set2 palette\n    color_map3, group_map3 = create_group_color_mapping(\n        items, group_size=3, palette_name='Set2', return_color_to_group=True\n    )\n    \n    # Example 4: viridis palette\n    color_map4, group_map4 = create_group_color_mapping(\n        items, group_size=3, palette_name='viridis', return_color_to_group=True\n    )\n    \n    # Plot all examples\n    palettes = [\n        ('Default Palette', color_map1, group_map1),\n        ('tab10 Palette', color_map2, group_map2),\n        ('Set2 Palette', color_map3, group_map3),\n        ('viridis Palette', color_map4, group_map4)\n    ]\n    \n    for i, (title, color_map, group_map) in enumerate(palettes):\n        ax = axes[i]\n        \n        # Plot bars\n        for j, item in enumerate(items):\n            ax.barh(0, 0.8, left=j, height=0.8, color=color_map[item], alpha=0.7)\n            if i == 0:  # Only add labels on the first plot\n                ax.text(j+0.4, 0, item, rotation=90, ha='center', va='bottom')\n        \n        # Add legend\n        \n        legend_elements = [Patch(facecolor=color, label=group) for color, group in group_map.items()]\n        ax.legend(handles=legend_elements, loc='upper center', ncol=len(group_map))\n        \n        ax.set_ylim(-0.5, 0.5)\n        ax.set_xlim(-0.5, len(items) - 0.5)\n        ax.set_yticks([])\n        ax.set_xticks([])\n        ax.set_title(title)\n    \n    plt.tight_layout()\n    \n    # Print example of hex colors from tab10\n    print(\"Example hex colors from tab10 palette:\")\n    for color in list(group_map2.keys())[:5]:\n        print(color)\n    \n    return fig, axes\n\n\ndemonstrate_group_mapping_palettes()\n\nExample hex colors from tab10 palette:\n#1f77b4\n#ff7f0e\n#2ca02c\n#d62728\n#9467bd\n\n\n(&lt;Figure size 864x720 with 4 Axes&gt;,\n array([&lt;Axes: title={'center': 'Default Palette'}&gt;,\n        &lt;Axes: title={'center': 'tab10 Palette'}&gt;,\n        &lt;Axes: title={'center': 'Set2 Palette'}&gt;,\n        &lt;Axes: title={'center': 'viridis Palette'}&gt;], dtype=object))\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nnorm_loading\n\n norm_loading (df)\n\n*Normalize datasets by equalizing the medians of all columns to a common target value.\nThis function implements a median normalization strategy that: 1. Calculates the median value for each column in the input dataframe 2. Computes a target value (the mean of all column medians) 3. Derives normalization factors to adjust each column to the target median 4. Applies these normalization factors to create a normalized dataset*\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data with controlled medians\n# Creating a dataset with 100 features (rows) and 6 samples (columns)\n# - 3 replicates for condition 1 (lower median)\n# - 3 replicates for condition 2 (higher median)\n\n# Number of features (e.g., proteins, genes)\nn_features = 100\n\n# Create condition 1 data (3 replicates with similar distribution)\ncondition1_rep1 = np.random.normal(loc=100, scale=25, size=n_features)\ncondition1_rep2 = np.random.normal(loc=95, scale=20, size=n_features)\ncondition1_rep3 = np.random.normal(loc=90, scale=22, size=n_features)\n\n# Create condition 2 data (3 replicates with higher median)\ncondition2_rep1 = np.random.normal(loc=150, scale=30, size=n_features)\ncondition2_rep2 = np.random.normal(loc=160, scale=28, size=n_features)\ncondition2_rep3 = np.random.normal(loc=155, scale=32, size=n_features)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'Cond1_Rep1': condition1_rep1,\n    'Cond1_Rep2': condition1_rep2,\n    'Cond1_Rep3': condition1_rep3,\n    'Cond2_Rep1': condition2_rep1,\n    'Cond2_Rep2': condition2_rep2,\n    'Cond2_Rep3': condition2_rep3\n})\n\n# Apply the normalization function\ndata_normalized = norm_loading(data)\n\n# Set up the figure for visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n\n# Color mapping for conditions\ncolors = ['#3498db', '#e74c3c']  # Blue for Condition 1, Red for Condition 2\ncondition_colors = {\n    'Cond1_Rep1': colors[0], 'Cond1_Rep2': colors[0], 'Cond1_Rep3': colors[0],\n    'Cond2_Rep1': colors[1], 'Cond2_Rep2': colors[1], 'Cond2_Rep3': colors[1]\n}\n\n# 1. Boxplot for raw data (before normalization)\nax1.set_title('Before Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data, ax=ax1, palette=condition_colors)\n# Get current tick positions\nax1_ticks = ax1.get_xticks()\nax1_labels = [label.get_text() for label in ax1.get_xticklabels()]\n# Set ticks and then ticklabels\nax1.set_xticks(ax1_ticks)\nax1.set_xticklabels(ax1_labels, rotation=45, ha='right')\nax1.set_ylabel('Value', fontsize=12)\nax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n# 2. Boxplot for normalized data\nax2.set_title('After Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data_normalized, ax=ax2, palette=condition_colors)\n# Get current tick positions\nax2_ticks = ax2.get_xticks()\nax2_labels = [label.get_text() for label in ax2.get_xticklabels()]\n# Set ticks and then ticklabels\nax2.set_xticks(ax2_ticks)\nax2.set_xticklabels(ax2_labels, rotation=45,ha='right')\nax2.set_ylabel('Normalized Value', fontsize=12)\nax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n\n# Create a custom legend for conditions\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor=colors[0], label='Condition 1'),\n    Patch(facecolor=colors[1], label='Condition 2')\n]\nfig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=2)\n\n# Adjust layout to make room for annotations\nplt.tight_layout(rect=[0, 0.1, 1, 0.9])\nplt.show()\n\nmedians [ 96.82609271  96.6821434   92.14930633 151.50472099 157.87481208\n 151.45188746]\ntarget 124.41482716043556\nnorm_facs [1.28493078 1.28684391 1.35014394 0.82119439 0.78806002 0.82148086]\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nquantileNormalize\n\n quantileNormalize (df_input, keep_na=True)\n\n*Perform quantile normalization on a pandas DataFrame.\nQuantile normalization is a technique that makes the distribution of values for each column identical by transforming the values to match the distribution of the mean of quantiles across all columns.\nAlgorithm: 1. Sort values in each column independently 2. Calculate the mean across rows of the sorted data (creating a reference distribution) 3. For each original value, assign the corresponding value from the reference distribution based on its rank in its original column*\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data with controlled medians\n# Creating a dataset with 100 features (rows) and 6 samples (columns)\n# - 3 replicates for condition 1 (lower median)\n# - 3 replicates for condition 2 (higher median)\n\n# Number of features (e.g., proteins, genes)\nn_features = 100\n\n# Create condition 1 data (3 replicates with similar distribution)\ncondition1_rep1 = np.random.normal(loc=100, scale=25, size=n_features)\ncondition1_rep2 = np.random.normal(loc=95, scale=20, size=n_features)\ncondition1_rep3 = np.random.normal(loc=90, scale=22, size=n_features)\n\n# Create condition 2 data (3 replicates with higher median)\ncondition2_rep1 = np.random.normal(loc=150, scale=30, size=n_features)\ncondition2_rep2 = np.random.normal(loc=160, scale=28, size=n_features)\ncondition2_rep3 = np.random.normal(loc=155, scale=32, size=n_features)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'Cond1_Rep1': condition1_rep1,\n    'Cond1_Rep2': condition1_rep2,\n    'Cond1_Rep3': condition1_rep3,\n    'Cond2_Rep1': condition2_rep1,\n    'Cond2_Rep2': condition2_rep2,\n    'Cond2_Rep3': condition2_rep3\n})\n\n# Apply the normalization function\ndata_normalized = quantileNormalize(data)\n\n# Set up the figure for visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n\n# Color mapping for conditions\ncolors = ['#3498db', '#e74c3c']  # Blue for Condition 1, Red for Condition 2\ncondition_colors = {\n    'Cond1_Rep1': colors[0], 'Cond1_Rep2': colors[0], 'Cond1_Rep3': colors[0],\n    'Cond2_Rep1': colors[1], 'Cond2_Rep2': colors[1], 'Cond2_Rep3': colors[1]\n}\n\n# 1. Boxplot for raw data (before normalization)\nax1.set_title('Before Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data, ax=ax1, palette=condition_colors)\n# Get current tick positions\nax1_ticks = ax1.get_xticks()\nax1_labels = [label.get_text() for label in ax1.get_xticklabels()]\n# Set ticks and then ticklabels\nax1.set_xticks(ax1_ticks)\nax1.set_xticklabels(ax1_labels, rotation=45, ha='right')\nax1.set_ylabel('Value', fontsize=12)\nax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n# 2. Boxplot for normalized data\nax2.set_title('After Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data_normalized, ax=ax2, palette=condition_colors)\n# Get current tick positions\nax2_ticks = ax2.get_xticks()\nax2_labels = [label.get_text() for label in ax2.get_xticklabels()]\n# Set ticks and then ticklabels\nax2.set_xticks(ax2_ticks)\nax2.set_xticklabels(ax2_labels, rotation=45,ha='right')\nax2.set_ylabel('Normalized Value', fontsize=12)\nax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n\n# Create a custom legend for conditions\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor=colors[0], label='Condition 1'),\n    Patch(facecolor=colors[1], label='Condition 2')\n]\nfig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=2)\n\n# Adjust layout to make room for annotations\nplt.tight_layout(rect=[0, 0.1, 1, 0.9])\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\nnorm_loading_TMT\n\n norm_loading_TMT (df)\n\n*Normalize TMT (Tandem Mass Tag) proteomics data to account for uneven sample loading.\nThis function performs total sum normalization, specifically designed for TMT-based multiplexed proteomics experiments where differences in total protein abundance between samples may be due to technical variations rather than biological differences.*\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data with controlled medians\n# Creating a dataset with 100 features (rows) and 6 samples (columns)\n# - 3 replicates for condition 1 (lower median)\n# - 3 replicates for condition 2 (higher median)\n\n# Number of features (e.g., proteins, genes)\nn_features = 100\n\n# Create condition 1 data (3 replicates with similar distribution)\ncondition1_rep1 = np.random.normal(loc=100, scale=25, size=n_features)\ncondition1_rep2 = np.random.normal(loc=95, scale=20, size=n_features)\ncondition1_rep3 = np.random.normal(loc=90, scale=22, size=n_features)\n\n# Create condition 2 data (3 replicates with higher median)\ncondition2_rep1 = np.random.normal(loc=150, scale=30, size=n_features)\ncondition2_rep2 = np.random.normal(loc=160, scale=28, size=n_features)\ncondition2_rep3 = np.random.normal(loc=155, scale=32, size=n_features)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'Cond1_Rep1': condition1_rep1,\n    'Cond1_Rep2': condition1_rep2,\n    'Cond1_Rep3': condition1_rep3,\n    'Cond2_Rep1': condition2_rep1,\n    'Cond2_Rep2': condition2_rep2,\n    'Cond2_Rep3': condition2_rep3\n})\n\n# Apply the normalization function\ndata_normalized = norm_loading_TMT(data)\n\n# Set up the figure for visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n\n# Color mapping for conditions\ncolors = ['#3498db', '#e74c3c']  # Blue for Condition 1, Red for Condition 2\ncondition_colors = {\n    'Cond1_Rep1': colors[0], 'Cond1_Rep2': colors[0], 'Cond1_Rep3': colors[0],\n    'Cond2_Rep1': colors[1], 'Cond2_Rep2': colors[1], 'Cond2_Rep3': colors[1]\n}\n\n# 1. Boxplot for raw data (before normalization)\nax1.set_title('Before Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data, ax=ax1, palette=condition_colors)\n# Get current tick positions\nax1_ticks = ax1.get_xticks()\nax1_labels = [label.get_text() for label in ax1.get_xticklabels()]\n# Set ticks and then ticklabels\nax1.set_xticks(ax1_ticks)\nax1.set_xticklabels(ax1_labels, rotation=45, ha='right')\nax1.set_ylabel('Value', fontsize=12)\nax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n# 2. Boxplot for normalized data\nax2.set_title('After Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data_normalized, ax=ax2, palette=condition_colors)\n# Get current tick positions\nax2_ticks = ax2.get_xticks()\nax2_labels = [label.get_text() for label in ax2.get_xticklabels()]\n# Set ticks and then ticklabels\nax2.set_xticks(ax2_ticks)\nax2.set_xticklabels(ax2_labels, rotation=45,ha='right')\nax2.set_ylabel('Normalized Value', fontsize=12)\nax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n\n# Create a custom legend for conditions\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor=colors[0], label='Condition 1'),\n    Patch(facecolor=colors[1], label='Condition 2')\n]\nfig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=2)\n\n# Adjust layout to make room for annotations\nplt.tight_layout(rect=[0, 0.1, 1, 0.9])\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\nires_norm\n\n ires_norm (df, exps_columns)\n\n*Implement Internal Reference Scaling (IRS) normalization for combining multiple TMT experiments.\nThis function normalizes and integrates data from multiple TMT experiments by: 1. Computing the sum of each protein‚Äôs intensity across all channels within each experiment 2. Calculating the geometric mean of these sums across experiments (reference value) 3. Deriving scaling factors to adjust each experiment to this reference 4. Applying an additional total sum normalization to the combined dataset*\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data with controlled medians\n# Creating a dataset with 100 features (rows) and 6 samples (columns)\n# - 3 replicates for condition 1 (lower median)\n# - 3 replicates for condition 2 (higher median)\n\n# Number of features (e.g., proteins, genes)\nn_features = 100\n\n# Create condition 1 data (3 replicates with similar distribution)\ncondition1_rep1 = np.random.normal(loc=100, scale=25, size=n_features)\ncondition1_rep2 = np.random.normal(loc=95, scale=20, size=n_features)\ncondition1_rep3 = np.random.normal(loc=90, scale=22, size=n_features)\n\n# Create condition 2 data (3 replicates with higher median)\ncondition2_rep1 = np.random.normal(loc=150, scale=30, size=n_features)\ncondition2_rep2 = np.random.normal(loc=160, scale=28, size=n_features)\ncondition2_rep3 = np.random.normal(loc=155, scale=32, size=n_features)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'Cond1_Rep1': condition1_rep1,\n    'Cond1_Rep2': condition1_rep2,\n    'Cond1_Rep3': condition1_rep3,\n    'Cond2_Rep1': condition2_rep1,\n    'Cond2_Rep2': condition2_rep2,\n    'Cond2_Rep3': condition2_rep3\n})\n\n# Apply the normalization function\ndata_normalized = ires_norm(data,[['Cond1_Rep1','Cond1_Rep2','Cond1_Rep3' ],['Cond2_Rep1','Cond2_Rep2','Cond2_Rep3' ]])\n\n# Set up the figure for visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n\n# Color mapping for conditions\ncolors = ['#3498db', '#e74c3c']  # Blue for Condition 1, Red for Condition 2\ncondition_colors = {\n    'Cond1_Rep1': colors[0], 'Cond1_Rep2': colors[0], 'Cond1_Rep3': colors[0],\n    'Cond2_Rep1': colors[1], 'Cond2_Rep2': colors[1], 'Cond2_Rep3': colors[1]\n}\n\n# 1. Boxplot for raw data (before normalization)\nax1.set_title('Before Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data, ax=ax1, palette=condition_colors)\n# Get current tick positions\nax1_ticks = ax1.get_xticks()\nax1_labels = [label.get_text() for label in ax1.get_xticklabels()]\n# Set ticks and then ticklabels\nax1.set_xticks(ax1_ticks)\nax1.set_xticklabels(ax1_labels, rotation=45, ha='right')\nax1.set_ylabel('Value', fontsize=12)\nax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n# 2. Boxplot for normalized data\nax2.set_title('After Normalization', fontsize=14, fontweight='bold')\nsns.boxplot(data=data_normalized, ax=ax2, palette=condition_colors)\n# Get current tick positions\nax2_ticks = ax2.get_xticks()\nax2_labels = [label.get_text() for label in ax2.get_xticklabels()]\n# Set ticks and then ticklabels\nax2.set_xticks(ax2_ticks)\nax2.set_xticklabels(ax2_labels, rotation=45,ha='right')\nax2.set_ylabel('Normalized Value', fontsize=12)\nax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n\n# Create a custom legend for conditions\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor=colors[0], label='Condition 1'),\n    Patch(facecolor=colors[1], label='Condition 2')\n]\nfig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05), ncol=2)\n\n# Adjust layout to make room for annotations\nplt.tight_layout(rect=[0, 0.1, 1, 0.9])\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\nclean_id\n\n clean_id (temp_id)\n\n\nsource\n\n\nmod_hist_legend\n\n mod_hist_legend (ax, title=False)\n\nCreates a cleaner legend for histogram plots by using line elements instead of patches. when using step Motivation: - Default histogram legends show rectangle patches which can be visually distracting - This function creates a more elegant legend with simple lines matching histogram edge colors - Positions the legend outside the plot to avoid overlapping with data\n\n# Create sample data for multiple distributions\nnp.random.seed(42)  # For reproducibility\ndata_a = np.random.normal(0, 1, 1000)\ndata_b = np.random.normal(3, 1.5, 1500)\n\n# Create a figure with 2 subplots side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left subplot: Default histogram legend\nax1.hist(data_a, bins=30, alpha=0.7, label='Distribution A', edgecolor='blue', histtype='step')\nax1.hist(data_b, bins=30, alpha=0.7, label='Distribution B', edgecolor='red', histtype='step')\nax1.set_title('Default Legend')\nax1.legend()  # Default legend\n\n# Right subplot: Modified histogram legend\nax2.hist(data_a, bins=30, alpha=0.7, label='Distribution A', edgecolor='blue', histtype='step')\nax2.hist(data_b, bins=30, alpha=0.7, label='Distribution B', edgecolor='red', histtype='step')\nax2.set_title('Modified Legend')\nmod_hist_legend(ax2, title='Distributions')  # Apply our function\n\n# Adjust layout to give space for the right-side legend\nplt.tight_layout()\nfig.subplots_adjust(right=0.85)\n\n# Display the figure\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\nclean_axes\n\n clean_axes (ax, offset=10)\n\nCustomizes a matplotlib axes by removing top and right spines, and creating a broken axis effect where x and y axes don‚Äôt touch.\n\n# Create sample data for multiple distributions\nnp.random.seed(42)  # For reproducibility\ndata_a = np.random.normal(0, 1, 1000)\ndata_b = np.random.normal(3, 1.5, 1500)\n\n# Create a figure with 2 subplots side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left subplot: Default histogram legend\nax1.hist(data_a, bins=30, alpha=0.7, label='Distribution A', edgecolor='blue', histtype='step')\nax1.hist(data_b, bins=30, alpha=0.7, label='Distribution B', edgecolor='red', histtype='step')\nax1.set_title('Default Legend')\nax1.legend()  # Default legend\n\n# Right subplot: Modified histogram legend\nax2.hist(data_a, bins=30, alpha=0.7, label='Distribution A', edgecolor='blue', histtype='step')\nax2.hist(data_b, bins=30, alpha=0.7, label='Distribution B', edgecolor='red', histtype='step')\nax2.set_title('Modified Axes')\nmod_hist_legend(ax2, title='Distributions')  # Apply our function\nclean_axes(ax2)\n# Adjust layout to give space for the right-side legend\nplt.tight_layout()\nfig.subplots_adjust(right=0.85)\n\n# Display the figure\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\nadd_desc\n\n add_desc (data, prot_to_desc)\n\n\nsource\n\n\nparse_fasta_file\n\n parse_fasta_file (fasta_file)\n\ncreate a dictionary of protein id to gene product using fasta file from tritrypDB\n\nsource\n\n\nget_scaled_df\n\n get_scaled_df (df)\n\n\nsource\n\n\nelbow_point\n\n elbow_point (values)\n\nFind the elbow point in a curve using the maximum curvature method.\n\n\n\n\nType\nDetails\n\n\n\n\nvalues\nlist\nThe y-values of the curve.\n\n\nReturns\nint\nThe index of the elbow point.\n\n\n\n\nsource\n\n\nkmeans_cluster_analysis\n\n kmeans_cluster_analysis (df, cluster_sizes, random_state=42,\n                          features=None, figsize=(12, 6),\n                          standardize=False, fill_na=False)\n\nPerform K-means clustering analysis on a pandas DataFrame and visualize the results with both normalized inertia and silhouette scores on the same plot.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npandas.DataFrame\n\nThe input data to cluster.\n\n\ncluster_sizes\nlist\n\nList of cluster sizes (k values) to evaluate.\n\n\nrandom_state\nint\n42\nRandom seed for reproducibility (default: 42).\n\n\nfeatures\nNoneType\nNone\nList of column names to use for clustering. If None, all columns are used.\n\n\nfigsize\ntuple\n(12, 6)\nFigure size for the output plot (default: (12, 6)).\n\n\nstandardize\nbool\nFalse\nWhether to standardize the features (default: False).\n\n\nfill_na\nbool\nFalse\nWhether to fill missing values with column means (default: False).\n\n\nReturns\ntuple\n\n(figure, inertia_values, silhouette_values) - The matplotlib figure object,the list of inertia values, and the list of silhouette scores.\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n# Create synthetic dataset with 4 natural clusters\nX, y = make_blobs(\n    n_samples=400, \n    centers=4, \n    cluster_std=0.8, \n    random_state=42\n)\n\n# Convert to DataFrame\ndf = pd.DataFrame(X, columns=['feature1', 'feature2'])\n\n# Print basic information about the dataset\nprint(f\"Dataset shape: {df.shape}\")\nprint(df.head())\n\n# Define the range of cluster sizes to test\ncluster_sizes = list(range(1, 11))  # Test k from 1 to 10\n\n# Run the kmeans cluster analysis\nfig, ax, inertia_values, silhouette_values = kmeans_cluster_analysis(\n    df=df,\n    cluster_sizes=cluster_sizes,\n    random_state=42,\n    standardize=True,  # Standardize the features\n    figsize=(12, 7)\n)\n\n# Now you can further customize the plot using the ax object\nax.set_facecolor('#f8f9fa')  # Light gray background\nax.set_title('K-means Clustering Analysis for Synthetic Data', fontsize=16, fontweight='bold')\n\n# Display the generated plot\nplt.show()\n\n# Print the actual optimal number of clusters (which should be 4 in this case)\nprint(\"\\nInertia values:\")\nfor k, inertia in zip(cluster_sizes, inertia_values):\n    print(f\"k={k}: {inertia:.2f}\")\n\nprint(\"\\nSilhouette scores:\")\nfor k, silhouette in zip(cluster_sizes, silhouette_values):\n    if k &gt; 1:  # Silhouette score not defined for k=1\n        print(f\"k={k}: {silhouette:.4f}\")\n\n# Create a scatter plot of the data with the optimal cluster assignment (k=4)\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df)\n\n# Fit KMeans with k=4\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\n\n# Create a scatter plot with cluster assignments using fig, ax\nfig, ax = plt.subplots(figsize=(10, 8))\nscatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.8)\nax.set_title('K-means Clustering Result (k=4)', fontsize=15)\nax.set_xlabel('Feature 1', fontsize=12)\nax.set_ylabel('Feature 2', fontsize=12)\nax.grid(True, linestyle='--', alpha=0.7)\nfig.tight_layout()\n\nplt.show()\n\n# Verify the implementation by comparing with manually calculated metrics\n# For k=4, calculate inertia manually\nmanual_inertia = 0\nfor i, point in enumerate(X_scaled):\n    centroid = kmeans.cluster_centers_[labels[i]]\n    manual_inertia += np.sum((point - centroid) ** 2)\n\nprint(f\"\\nVerification for k=4:\")\nprint(f\"KMeans inertia: {kmeans.inertia_:.4f}\")\nprint(f\"Manually calculated inertia: {manual_inertia:.4f}\")\n\nDataset shape: (400, 2)\n   feature1  feature2\n0 -9.862671  8.727358\n1 -4.604994  9.671808\n2 -9.034922  7.105344\n3  5.419975  1.855524\n4  5.096591  2.881622\nStandardizing features.\n\nInertia values:\nk=1: 800.00\nk=2: 417.20\nk=3: 89.69\nk=4: 15.00\nk=5: 13.43\nk=6: 11.91\nk=7: 10.48\nk=8: 9.25\nk=9: 8.48\nk=10: 7.65\n\nSilhouette scores:\nk=2: 0.5702\nk=3: 0.7638\nk=4: 0.8403\nk=5: 0.7040\nk=6: 0.5770\nk=7: 0.4511\nk=8: 0.3408\nk=9: 0.3458\nk=10: 0.3538\n\nVerification for k=4:\nKMeans inertia: 14.9956\nManually calculated inertia: 14.9956",
    "crumbs": [
      "Semi Random Collection of functions"
    ]
  },
  {
    "objectID": "correlation_utilities.html",
    "href": "correlation_utilities.html",
    "title": "Correlation Utilities for Replicated Samples",
    "section": "",
    "text": "Overview\nThis notebook contains utility functions and workflows for analyzing correlations between replicated samples in experimental data.\nsource",
    "crumbs": [
      "Correlation Utilities for Replicated Samples"
    ]
  },
  {
    "objectID": "correlation_utilities.html#test-calss",
    "href": "correlation_utilities.html#test-calss",
    "title": "Correlation Utilities for Replicated Samples",
    "section": "Test Calss",
    "text": "Test Calss\n\n# Set plotting style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Example data creation\nnp.random.seed(42)  # For reproducibility\n\n# Create a sample dataframe with replicated measurements\n# Let's say we have 100 genes measured across 2 sample groups with 3 replicates each\nn_genes = 100\n\n# Sample 1 replicates with moderate variability\ns1_rep1 = np.random.lognormal(mean=2, sigma=0.2, size=n_genes)\ns1_rep2 = np.random.lognormal(mean=2, sigma=0.2, size=n_genes)\ns1_rep3 = np.random.lognormal(mean=2, sigma=0.2, size=n_genes)\n\n# Sample 2 replicates with higher variability\ns2_rep1 = np.random.lognormal(mean=1.5, sigma=0.4, size=n_genes)\ns2_rep2 = np.random.lognormal(mean=1.5, sigma=0.4, size=n_genes)\ns2_rep3 = np.random.lognormal(mean=1.5, sigma=0.4, size=n_genes)\n\n# Create a third sample group with very high variability (for demonstration)\ns3_rep1 = np.random.lognormal(mean=2, sigma=0.8, size=n_genes)\ns3_rep2 = np.random.lognormal(mean=2, sigma=0.8, size=n_genes)\n\n# Create dataframe\ndata = pd.DataFrame({\n    'S1-Replica1': s1_rep1,\n    'S1-Replica2': s1_rep2,\n    'S1-Replica3': s1_rep3,\n    'S2-Replica1': s2_rep1,\n    'S2-Replica2': s2_rep2,\n    'S2-Replica3': s2_rep3,\n    'S3-Replica1': s3_rep1,\n    'S3-Replica2': s3_rep2\n})\n\n# Define sample groups\nsample_groups = ['mysample_1', 'mysample_1', 'mysample_1', \n                'mysample_2', 'mysample_2', 'mysample_2',\n                'mysample_3', 'mysample_3']\n\n# Initialize analyzer with data\nanalyzer = ReplicateAnalyzer(data, sample_groups)\n\n# Calculate coefficient of variation (average across all measurements)\ncv_results = analyzer.calculate_coefficient_of_variation()\nprint(\"Mean Coefficient of Variation Results:\")\nprint(cv_results)\n\n# Visualize the mean CV results as a bar plot\nprint(\"\\nGenerating mean CV bar plot...\")\nfig1 = analyzer.plot_coefficient_of_variation(\n    title=\"Mean CV Comparison Between Sample Groups\",\n    figsize=(8, 5)\n)\n\n# Get mapping of sample groups to column names\nmapping = analyzer.get_sample_group_mapping()\nprint(\"\\nSample Group Mapping:\")\nfor group, columns in mapping.items():\n    print(f\"{group}: {columns}\")\n\n# Now use the new CV boxplot functionality to show the distribution of CV values\nprint(\"\\nCalculating CV distribution...\")\ncv_distribution = analyzer.calculate_cv_distribution(exclude_zeros=True)\n\n# Display summary statistics of the CV distribution\nprint(\"\\nCV Distribution Summary Statistics:\")\nprint(cv_distribution.describe())\n\n# Create the CV boxplot\nprint(\"\\nGenerating CV distribution boxplot...\")\nfig2 = analyzer.plot_cv_boxplot(\n    min_y=0,  # Minimum y-axis value\n    max_y=140,  # Maximum y-axis value\n    figsize=(8, 5),\n    color_palette=\"viridis\",\n    display_median=True,\n    reference_line=20,  \n    title=\"Distribution of Coefficient of Variation Across Sample Groups\"\n)\n\n# Show plots\nplt.tight_layout()\nplt.show()\n\nMean Coefficient of Variation Results:\n{'mysample_1': 17.080592807021713, 'mysample_2': 31.238071697274144, 'mysample_3': 59.19997632844498}\n\nGenerating mean CV bar plot...\n\nSample Group Mapping:\nmysample_1: ['S1-Replica1', 'S1-Replica2', 'S1-Replica3']\nmysample_2: ['S2-Replica1', 'S2-Replica2', 'S2-Replica3']\nmysample_3: ['S3-Replica1', 'S3-Replica2']\n\nCalculating CV distribution...\n\nCV Distribution Summary Statistics:\n       mysample_1  mysample_2  mysample_3\ncount  100.000000  100.000000  100.000000\nmean    17.080593   31.238072   59.199976\nstd      9.291617   17.255365   34.850935\nmin      1.571174    4.294527    0.111769\n25%     10.560251   16.585752   30.218940\n50%     15.791861   29.000638   52.498173\n75%     21.370398   39.970172   92.043385\nmax     49.541067   72.443491  125.483679\n\nGenerating CV distribution boxplot...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\nconcordance_correlation_coefficient\n\n concordance_correlation_coefficient (y_true, y_pred)\n\n*Calculate the concordance correlation coefficient (CCC).\nThe concordance correlation coefficient measures the agreement between two variables, ranging from -1 to 1, where 1 represents perfect agreement.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ny_true\narray-like\nGround truth values.\n\n\ny_pred\narray-like\nPredicted values.\n\n\nReturns\nfloat\nConcordance correlation coefficient value between -1 and 1.A value of 1 indicates perfect agreement between the true and predicted values.\n\n\n\n\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nconcordance_correlation_coefficient(y_true, y_pred)\n#0.9767891682785301\n\n0.9767891682785301",
    "crumbs": [
      "Correlation Utilities for Replicated Samples"
    ]
  },
  {
    "objectID": "proteomics.html",
    "href": "proteomics.html",
    "title": "üî¨ Proteomics Quality Control: Beyond the Basics",
    "section": "",
    "text": "#pl.read_csv??\n'''\npath ='/Volumes/dgh-lab/PROTEOMICS_DATA_DUMP/020_2025_DUN_DH/DIA-NN/020_2025_DUN_DH/'\nfname = '020_2025_DUN_DH-report.parquet'\ntmp = pl.read_parquet(os.path.join(path,fname)).to_pandas()\ntmp['RT.Diff']=tmp['RT.Stop']-tmp['RT.Start']\ntmp['RT.Bin']=tmp['RT'].astype(int)\nrt_dataset = tmp[['Run','RT','RT.Start','RT.Stop','RT.Diff','RT.Bin','Q.Value','Ms1.Area','Precursor.Quantity']]\nrt_dataset = rt_dataset.sort_values(['Run','RT'])\nrt_dataset['Significant']=(rt_dataset['Q.Value']&lt;0.01).astype(int)\nrt_dataset.head()\nrt_dataset['Significant'].value_counts()\nrt_dataset[rt_dataset['Run']=='020_2025-DUN_DH-GB-2T1-A'].plot(kind='scatter',x='RT',y='RT.Diff')\n# evantually track\nRT vs Precursor.Quantity / Ms1.Area\nRT vs RT.Diff\n'''\nprint(1)\n# working in progress\n\n1\nsource",
    "crumbs": [
      "üî¨ Proteomics Quality Control: Beyond the Basics"
    ]
  },
  {
    "objectID": "proteomics.html#example",
    "href": "proteomics.html#example",
    "title": "üî¨ Proteomics Quality Control: Beyond the Basics",
    "section": "Example",
    "text": "Example\npath ='/Volumes/dgh-lab/PROTEOMICS_DATA_DUMP/020_2025_DUN_DH/DIA-NN/020_2025_DUN_DH/'\nfname = '020_2025_DUN_DH-report.parquet'\ntmp = pl.read_parquet(os.path.join(path,fname))\ncount_data= prepare_data_DiaNN(tmp)\ncount_data=count_data.to_pandas()\ncount_data.head()\nplot_proteomics_run(count_data)",
    "crumbs": [
      "üî¨ Proteomics Quality Control: Beyond the Basics"
    ]
  },
  {
    "objectID": "mis_val_utility.html",
    "href": "mis_val_utility.html",
    "title": "Missing Values Utilities",
    "section": "",
    "text": "Comprehensive tools for visualizing and analyzing missing data patterns.\nOverview This notebook is part of the project_utility package and provides tools for visualizing and analyzing missing data in pandas DataFrames. The tools focus on providing multiple complementary views of missing data patterns to help guide data cleaning and preprocessing. Imports",
    "crumbs": [
      "Missing Values Utilities"
    ]
  },
  {
    "objectID": "mis_val_utility.html#missing-values-analysis-class",
    "href": "mis_val_utility.html#missing-values-analysis-class",
    "title": "Missing Values Utilities",
    "section": "Missing Values Analysis Class",
    "text": "Missing Values Analysis Class\n\nThis class provides a unified interface for visualizing missing data patterns.\n\n\nsource\n\nMissingValuesAnalyzer\n\n MissingValuesAnalyzer (df:pandas.core.frame.DataFrame)\n\nA class for visualizing and analyzing missing data in pandas DataFrames.\n\n# Create a DataFrame with missing values\ndef create_sample_df(n_rows=1000, n_cols=50, missing_fraction=0.2, random_seed=42):\n    \"\"\"Create a sample DataFrame with controlled missing values.\"\"\"\n    np.random.seed(random_seed)\n    \n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.lognormal(0, 1, size=(n_rows, n_cols)))\n    \n    # Add column names\n    df.columns = [f'feature_{i}' for i in range(n_cols)]\n    \n    # Add row names\n    df.index = [f'sample_{i}' for i in range(n_rows)]\n    \n    # Create patterns of missing values\n    \n    # 1. Completely random missing values\n    mask_random = np.random.random(size=df.shape) &lt; (missing_fraction / 2)\n    \n    # 2. Structured missing values - some features have more missing values\n    mask_cols = np.zeros(df.shape, dtype=bool)  # Explicitly use boolean dtype\n    cols_with_missing = np.random.choice(n_cols, size=int(n_cols * 0.3), replace=False)\n    mask_cols[:, cols_with_missing] = np.random.random(size=(n_rows, len(cols_with_missing))) &lt; missing_fraction\n    \n    # 3. Structured missing values - some samples have more missing values\n    mask_rows = np.zeros(df.shape, dtype=bool)  # Explicitly use boolean dtype\n    rows_with_missing = np.random.choice(n_rows, size=int(n_rows * 0.2), replace=False)\n    mask_rows[rows_with_missing, :] = np.random.random(size=(len(rows_with_missing), n_cols)) &lt; missing_fraction\n    \n    # 4. Value-dependent missing - higher values more likely to be missing\n    mask_value = (df &gt; df.mean().mean() * 1.5) & (np.random.random(size=df.shape) &lt; 0.5)\n    \n    # Combine masks\n    combined_mask = mask_random | mask_cols | mask_rows | mask_value\n    \n    # Set values to NaN based on the mask\n    df_with_missing = df.copy()\n    df_with_missing[combined_mask] = np.nan\n    \n    return df_with_missing\n\n\n# Create a sample dataframe\nsample_df = create_sample_df(n_rows=200, n_cols=10)\n\n# Check the overall missing percentage\ntotal_missing_pct = sample_df.isna().sum().sum() / sample_df.size * 100\nprint(f\"Total missing values: {total_missing_pct:.2f}%\")\n\nTotal missing values: 26.50%",
    "crumbs": [
      "Missing Values Utilities"
    ]
  },
  {
    "objectID": "mis_val_utility.html#example-usage",
    "href": "mis_val_utility.html#example-usage",
    "title": "Missing Values Utilities",
    "section": "Example Usage",
    "text": "Example Usage\n\nLet‚Äôs create some sample data with missing values and demonstrate the utility:\n\n\nprint(1)\n\n1\n\n\n\n# Create a missing values analyzer\nmv_analyzer = MissingValuesAnalyzer(sample_df)\n\n# Generate the dashboard\nfig, axes, summary = mv_analyzer.plot_missing_dashboard()",
    "crumbs": [
      "Missing Values Utilities"
    ]
  },
  {
    "objectID": "mis_val_utility.html#group-based-minprob-imputation-for-proteomics-data",
    "href": "mis_val_utility.html#group-based-minprob-imputation-for-proteomics-data",
    "title": "Missing Values Utilities",
    "section": "Group-Based MinProb Imputation for Proteomics Data",
    "text": "Group-Based MinProb Imputation for Proteomics Data\n\nOverview\n\nThis utility implements a specialized imputation strategy for proteomics data that handles missing values in a biologically meaningful way. The approach is based on the MinProb imputation strategy commonly used in proteomics analysis, but with modifications to work on a condition group basis.\n\nImputation Strategy\nThe imputation follows these key principles: - Condition-Based Statistics: For each experimental condition group, the algorithm computes distribution statistics by pooling all values across replicates and proteins. - Selective Imputation: Missing values are only imputed when all measurements for a protein within a specific condition group are missing. This approach is ideal for handling proteins that are completely undetected in certain conditions. - Low-Intensity Replacement: Following the MinProb philosophy, missing values are imputed as low-intensity signals, drawn from a normal distribution centered at a low quantile of the observed values.\n\nsource\n\n\ngroup_based_minprob_impute\n\n group_based_minprob_impute (df, group_vector, quantile=0.0001,\n                             sd_factor=0.2, random_state=None)\n\n*Group-based MinProb imputation for proteomics data. First computes quantile distribution statistics for each condition group by merging all values within the condition. Then imputes missing values only for rows where all values in a particular condition are missing by drawing random values from the calculated distribution.\nParameters: df (pd.DataFrame): Input DataFrame in log space (rows: proteins, columns: samples) group_vector (list): Vector indicating group membership of each column quantile (float): Quantile to use as the center for the imputation (default: 0.01) sd_factor (float): Factor to multiply the standard deviation for noise level (default: 0.2) random_state (int, optional): Random seed for reproducibility\nReturns: pd.DataFrame: DataFrame with imputed missing values.*",
    "crumbs": [
      "Missing Values Utilities"
    ]
  },
  {
    "objectID": "mis_val_utility.html#usage-example",
    "href": "mis_val_utility.html#usage-example",
    "title": "Missing Values Utilities",
    "section": "Usage example",
    "text": "Usage example\n\n# Create a toy DataFrame in log2 space\nnp.random.seed(42)\ndata = np.log2(np.random.rand(10, 5) * 1e4)\ndf_example = pd.DataFrame(data, columns=[f'Sample{i+1}' for i in range(5)])\n\n# Define condition groups: first 3 columns are group 1, last 2 columns are group 2\ngroups = [1, 1, 1, 2, 2]\n\n# Introduce missing values by groups\n# Make all values in group 1 missing for row 2\ndf_example.iloc[2, 0:3] = np.nan\n\n# Make all values in group 2 missing for row 5\ndf_example.iloc[5, 3:5] = np.nan\n\nprint(\"Before imputation:\")\nprint(df_example)\nprint(\"\\nGroup vector:\", groups)\n\n# Apply the imputation\nimputed_df = group_based_minprob_impute(df_example, groups, random_state=42)\n\nprint(\"\\nAfter imputation:\")\nprint(imputed_df)\n#selection_normed = group_based_minprob_impute(np.log2(df).copy(), group_vector=[1,1,1,2,2,3,3,4,4,4,5,5,5])\n\nBefore imputation:\n     Sample1    Sample2    Sample3    Sample4    Sample5\n0  11.870905  13.214796  12.837616  12.547518  10.607503\n1  10.607280   9.181987  13.080445  12.553425  12.789682\n2        NaN        NaN        NaN  11.052154  10.828335\n3  10.840813  11.571005  12.357432  12.076632  11.507939\n4  12.578969  10.445986  11.512467  11.839054  12.155040\n5  12.938800  10.963429  12.328211        NaN        NaN\n6  12.568775  10.735760   9.345441  13.212018  13.237258\n7  12.980849  11.572765   9.931803  12.740272  12.103788\n8  10.253118  12.273728   8.425783  13.150573  11.337510\n9  12.693753  11.605994  12.344485  12.416561  10.852174\n\nGroup vector: [1, 1, 1, 2, 2]\nComputing condition statistics:\n  Condition 1: 27 valid values, center=8.4277, noise_sd=0.2576\n  Condition 2: 18 valid values, center=10.6079, noise_sd=0.1666\n\nImputing missing values:\n  Imputed missing values for row 2 (index: 2)\n  Imputed missing values for row 5 (index: 5)\n\nImputation complete: 5 values imputed across 10 rows\n\nAfter imputation:\n     Sample1    Sample2    Sample3    Sample4    Sample5\n0  11.870905  13.214796  12.837616  12.547518  10.607503\n1  10.607280   9.181987  13.080445  12.553425  12.789682\n2   8.427749   8.392130   8.427749  11.052154  10.828335\n3  10.840813  11.571005  12.357432  12.076632  11.507939\n4  12.578969  10.445986  11.512467  11.839054  12.155040\n5  12.938800  10.963429  12.328211  10.607878  10.568862\n6  12.568775  10.735760   9.345441  13.212018  13.237258\n7  12.980849  11.572765   9.931803  12.740272  12.103788\n8  10.253118  12.273728   8.425783  13.150573  11.337510\n9  12.693753  11.605994  12.344485  12.416561  10.852174",
    "crumbs": [
      "Missing Values Utilities"
    ]
  },
  {
    "objectID": "diff_expr_utility.html",
    "href": "diff_expr_utility.html",
    "title": "Volcano and MA Plot Visualization for Omics Data",
    "section": "",
    "text": "The PlotData class handles loading data from CSV files and mapping columns to standard names.\n\n\nsource\n\n\n\n PlotData (file_path, column_mapping=None, log_fdr=False,\n           highlight_ids=None)\n\nInitialize the PlotData class for various omics experiments.\n\nempty_data_dict\n\n{'log2fc': None,\n 'log10_fdr': None,\n 'avg_intensity': None,\n 'id': None,\n 'description': None,\n 'fdr': None,\n 'highlight_indices': None}",
    "crumbs": [
      "Volcano and MA Plot Visualization for Omics Data"
    ]
  },
  {
    "objectID": "diff_expr_utility.html#data-loading-and-processing",
    "href": "diff_expr_utility.html#data-loading-and-processing",
    "title": "Volcano and MA Plot Visualization for Omics Data",
    "section": "",
    "text": "The PlotData class handles loading data from CSV files and mapping columns to standard names.\n\n\nsource\n\n\n\n PlotData (file_path, column_mapping=None, log_fdr=False,\n           highlight_ids=None)\n\nInitialize the PlotData class for various omics experiments.\n\nempty_data_dict\n\n{'log2fc': None,\n 'log10_fdr': None,\n 'avg_intensity': None,\n 'id': None,\n 'description': None,\n 'fdr': None,\n 'highlight_indices': None}",
    "crumbs": [
      "Volcano and MA Plot Visualization for Omics Data"
    ]
  },
  {
    "objectID": "diff_expr_utility.html#visualization-function",
    "href": "diff_expr_utility.html#visualization-function",
    "title": "Volcano and MA Plot Visualization for Omics Data",
    "section": "Visualization Function",
    "text": "Visualization Function\n\nNow we‚Äôll define the function to generate the side-by-side volcano and MA plots:\n\n\nsource\n\ncreate_volcano_ma_plots\n\n create_volcano_ma_plots (plot_data, plot_title='Volcano and MA Plots',\n                          width=600, height=800)\n\nCreate one-on-top-of-the-other volcano and MA plots using Plotly.",
    "crumbs": [
      "Volcano and MA Plot Visualization for Omics Data"
    ]
  },
  {
    "objectID": "diff_expr_utility.html#example-usage",
    "href": "diff_expr_utility.html#example-usage",
    "title": "Volcano and MA Plot Visualization for Omics Data",
    "section": "Example Usage",
    "text": "Example Usage\n\nLet‚Äôs demonstrate how to use these functions with a real dataset:\n\n\ncolumn_mapping = {\n    'log2fc': 'logFC',            # logFC column from your data\n    'fdr': 'FDR',                # FDR column from your data\n    'avg_intensity': 'log_AveExpr', # log_AveExpr column from your data\n    'id': 'Gene_id',            # Gene_acc column from your data\n    'description': 'Desc'         # Desc column from your data\n}\n\nfile_path = '../tests/volcano_plots/for_web_limma_WT-C3.csv.zip'\n# Create the PlotData instance\nplot_data = PlotData(file_path, column_mapping, highlight_ids=['Blasticidin','Puromycin'])\n\n# Print some information about the loaded data\nprint(f\"Data loaded: {plot_data.data is not None}\")\nprint(f\"Number of rows: {len(plot_data.data) if plot_data.data is not None else 0}\")\nprint(f\"First few IDs: {plot_data.id[:5] if hasattr(plot_data, 'id') else 'Not loaded'}\")\nprint(f\"Highlight Indices: {plot_data.highlight_indices}\")\n# Quick access to all plotting data\nplotting_data = plot_data.get_data_for_plotting()\n\nData loaded: True\nNumber of rows: 6205\nFirst few IDs: ['Blasticidin' 'Puromycin' 'Tb05.5K5.100;Tb927.5.4450'\n 'Tb05.5K5.110;Tb927.5.4460' 'Tb05.5K5.120;Tb927.5.4470']\nHighlight Indices: [0, 1]\n\n\n\n#!rm iframe_figures/*\n\n\nfig = create_volcano_ma_plots(\n    plot_data,\n    plot_title=\"Differential Expression Analysis: Sample vs Control\"\n)\n\n\nfig.show('iframe')",
    "crumbs": [
      "Volcano and MA Plot Visualization for Omics Data"
    ]
  }
]